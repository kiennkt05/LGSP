import torch  
import torch.nn as nn  
import torch.nn.functional as F  

from utils import *
from timm.models import create_model  
import matplotlib.pyplot as plt  

class ViT_MYNET(nn.Module):
    def __init__(self, args, mode=None):
        super().__init__()
        self.mode = mode
        self.args = args
        
        if self.args.dataset in ['cifar100', 'cub200', 'mini_imagenet', 'FGVCAircraft', 'iNF200', 'air']:
            self.num_features = 768
        
        self.encoder = create_model("vit_base_patch16_224_in21k", pretrained=True, num_classes=args.num_classes,
                                drop_rate=0., drop_path_rate=0., drop_block_rate=None)
        
        # Classifier Head as a Fully Connected Layer
        self.classifier_head = nn.Linear(self.num_features, self.args.num_classes, bias=False)
        
        self.seen_classes = args.base_class
        self.way = args.way
        self.base_class = args.base_class

        self.Prompt_Token_num = args.Prompt_Token_num
        self.Prompt_Tokens = nn.Parameter(torch.zeros(len(self.encoder.blocks), self.Prompt_Token_num, self.encoder.embed_dim))
        torch.nn.init.normal_(self.Prompt_Tokens, mean=0, std=0.1)
        # torch.nn.init.uniform_(self.Prompt_Tokens, -1, 1)

        # Random dropout (zeroing out elements with probability Dropout_Prompt)
        # acts as a regularization technique to prevent overfitting
        # to specific noise patterns generated by the prompt networks
        self.prompt_dropout = torch.nn.Dropout(self.args.Dropout_Prompt)

        self.first_kernel_size = args.first_kernel_size
        self.second_kernel_size = args.second_kernel_size
        
        def build_prompt_module():
            return nn.Sequential(
                nn.Conv2d(3, args.prompt_hid_dim, self.first_kernel_size, stride=1, padding=int((self.first_kernel_size - 1) / 2)),
                nn.ReLU(),
                nn.Conv2d(args.prompt_hid_dim, 3, self.second_kernel_size, stride=1, padding=int((self.second_kernel_size - 1) / 2))
            )

        self.prompt_generators = nn.ModuleList()

        if self.args.pixel_prompt == "YES":
            pool_size = self.args.pool_size
            self.prompt_generators = nn.ModuleList(
                build_prompt_module() for _ in range(pool_size)
            )
            self.num_prompt_generators = pool_size
        else:
            self.num_prompt_generators = 0
            
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        if self.args.Frequency_mask:
            max_radius = torch.sqrt(torch.tensor((224 / 2) ** 2 + (224 / 2) ** 2)).item()
            self.radii = torch.linspace(0, max_radius, steps=self.args.num_r)
            weights_init = torch.normal(mean=0, std=10, size=(self.args.num_r,))
            self.weights = nn.Parameter(weights_init)

            if self.args.adaptive_weighting:
                self.alpha = nn.Parameter(torch.tensor(0.5, requires_grad=True))
                self.beta = nn.Parameter(torch.tensor(0.5, requires_grad=True))

    def update_seen_classes(self, new_classes):
        print('new classes for this session:\n', new_classes)
        self.mask = torch.zeros(self.args.num_classes,device='cuda')
        self.mask[:self.seen_classes]=-torch.inf
        self.seen_classes += len(new_classes)
    
    def encode(self, x):
        x = self.encoder.forward_features(x)[:,0]
        return x
    
    def prompt_encode(self, img):
        x = self.encoder.patch_embed(img) # (batch_size, 196, embed_dim)
        ex_cls = self.encoder.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat([ex_cls,x],dim=1)
        x = self.encoder.pos_drop(x + self.encoder.pos_embed)

        Prompt_Token_num = self.Prompt_Tokens.shape[1]
        # [depth, Prompt_Token_num, emb]

        for i in range(len(self.encoder.blocks)):
            # print(len(self.encoder.blocks)) # 12
            # concatenate Prompt_Tokens
            # 拿出第i个并增加一个维度[1, Prompt_Token_num, emb]
            Prompt_Tokens = self.Prompt_Tokens[i].unsqueeze(0)
            # firstly concatenate
            # expand之后是[batch_size, Prompt_Token_num, emb]，之后和x进行cat
            x = torch.cat((x, Prompt_Tokens.expand(x.shape[0], -1, -1)), dim=1)
            # 此时x的维度是[batch_size, seq_len + Prompt_Token_num, emb]
            num_tokens = x.shape[1]
            # lastly remove, a genius trick
            x = self.encoder.blocks[i](x)[:, :num_tokens - Prompt_Token_num]
            # print(x.shape) # [64,197,768]
            # 通过这一层，再在处理结束时移除它们

        # print('哈哈')
        # print(Prompt_Tokens)
        x = self.encoder.norm(x)
        x=x[:, 0, :]
        return x
    
    # Define a function to calculate cosine similarity, focusing more on local similarity.
    def cosine_similarity(self, a, b):
        # Normalize the vectors
        a_norm = F.normalize(a, dim=1)  # [batch_size, channels, h, w]
        b_norm = F.normalize(b, dim=1)  # [batch_size, channels, h, w]
        # Calculate the dot product
        return torch.sum(a_norm * b_norm, dim=1, keepdim=True)  # [batch_size, 1, h, w]
    
    def get_prompts(self, x, session=-1):
        res = {}  # Initialize res as an empty dictionary
        if self.args.pixel_prompt == "YES":
            prompts_list = []
            for prompt_net in self.prompt_generators:
                prompts_list.append(self.prompt_dropout(prompt_net(x)))

            self.num_prompt_generators = len(prompts_list)

            # Use point-wise convolution to increase the channel dimension
            # Feature normalization, such as BatchNorm or GroupNorm, to reduce redundant information
            # Perform softmax on the branches
            similarities_list = [self.cosine_similarity(x, prompt) for prompt in prompts_list]  # Each element is [batch_size, 1, h, w]

            # Concatenate all similarities and perform softmax normalization
            similarities = torch.cat(similarities_list, dim=1)  # [batch_size, 20, h, w]
            # print(similarities.shape)
            # similarities = similarities * self.args.temperature
            weights = F.softmax(similarities, dim=1)  # [batch_size, 20, h, w]
            
            # if session == 7 or session == 8 or session == 9:
            #     # Sample 49: Max = 0.18, Min = 0.01
                
            #     # 遍历第一个维度 (shape[0])，对于每个样本
            #     for i in range(weights.shape[0]):
            #         # 获取每个样本的最大值和最小值，并保留两位小数
            #         max_val = round(weights[i].max().item(), 2)
            #         min_val = round(weights[i].min().item(), 2)
            #         # 输出最大值和最小值
            #         print(f"Sample {i}: Max = {max_val}, Min = {min_val}")
            # 堆叠 prompts 并进行加权求和
            prompts = torch.stack(prompts_list, dim=1)  # [batch_size, 10, channels, h, w]
            weighted_prompt = torch.sum(weights.unsqueeze(2) * prompts, dim=1)  # [batch_size, channels, h, w]
            prompts = weighted_prompt


            # # Average the branches
            # # Stack prompts, get shape: [batch_size, num_prompts, channels, h, w]
            # prompts = torch.stack(prompts_list, dim=1)  # [batch_size, num_prompts, channels, h, w]
            # # Directly take the mean of the num_prompts dimension
            # weighted_prompt = prompts.mean(dim=1)  # [batch_size, channels, h, w]
            # # Update prompts
            # prompts = weighted_prompt

        res['prompts'] = prompts
        return res
    
    # (0) Save the original image after de-normalization
    def save_batch_as_images(self, input_tensor, folder_path="output_images", file_prefix="image"):
        import os
        
        # Ensure the folder exists
        os.makedirs(folder_path, exist_ok=True)
        
        # Check the dimension of the input tensor
        if input_tensor.dim() != 4 or input_tensor.size(1) != 3:
            raise ValueError("The dimension of the input tensor must be [Batch_size, 3, h, w]")
        
        # Clip the tensor to the [0, 1] range
        input_clamped = input_tensor.clip(0, 1)
        
        # Iterate over each image in the Batch
        batch_size = input_tensor.size(0)
        for i in range(batch_size):
            # Extract a single image [3, h, w] -> [h, w, 3]
            image = input_clamped[i].permute(1, 2, 0).cpu().numpy()
            
            # Save as an image file
            file_path = os.path.join(folder_path, f"{file_prefix}_{i}.png")
            plt.imsave(file_path, image)
            print(f"Saved: {file_path}")
    
    # (1) Save 10 masks
    def save_ring_masks_as_images(self, ring_masks, save_dir):
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        num_rings = ring_masks.shape[0]
        
        for i in range(num_rings):
            # Get the i-th mask
            mask = ring_masks[i].detach().cpu().numpy()  # Convert to NumPy array

            # Draw the image
            plt.imshow(mask, cmap='gray')
            plt.axis('off')  # Do not display the coordinates

            # Save the image
            plt.savefig(os.path.join(save_dir, f'ring_mask_{i}.png'), bbox_inches='tight', pad_inches=0)
            plt.close()  # Close the image, so that it does not overlap with the next one
    
    # (2) Save the entire mask after weighting
    def save_weighted_mask(self, weighted_ring_masks, save_dir):
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        # Ensure the mask is on CPU and convert to NumPy format
        weighted_mask = weighted_ring_masks.detach().cpu().numpy()

        # Draw and save the total mask
        plt.imshow(weighted_mask, cmap='gray')
        plt.axis('off')  # Do not display the coordinates
        plt.savefig(os.path.join(save_dir, 'weighted_mask.png'), bbox_inches='tight', pad_inches=0)
        plt.close()

    # (3) Save the image of the entire mask added to the frequency domain
    def save_frequency_domain_image(self, fft_selected, save_dir, filename="frequency_domain.png"):
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        # Calculate the magnitude of the frequency domain (magnitude)
        magnitude = torch.abs(fft_selected).mean(dim=1)  # Take the average of the channel dimension, shape: [Batch_size, h, w]
        magnitude = magnitude.detach().cpu().numpy()  # Convert to NumPy format

        # Normalize to the [0, 1] range
        magnitude_normalized = (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min())

        # Save the frequency domain image of each sample
        for i in range(magnitude_normalized.shape[0]):
            plt.imshow(magnitude_normalized[i], cmap='gray')
            plt.axis('off')  # Do not display the coordinates
            plt.savefig(os.path.join(save_dir, f"{filename}_sample_{i}.png"), bbox_inches='tight', pad_inches=0)
            plt.close()

    # (4) Save the image returned to the spatial region
    def save_spatial_domain_images(self, ifft_selected, save_dir):
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        # Convert to NumPy format
        images = ifft_selected.detach().cpu().numpy()  # Convert to NumPy format, shape: [Batch_size, 3, h, w]

        # Save each image
        for i in range(images.shape[0]):
            image = images[i].transpose(1, 2, 0)  # Convert to [h, w, 3] format
            image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]

            # Save the image
            plt.imshow(image)
            plt.axis('off')  # Do not display the coordinates
            plt.savefig(os.path.join(save_dir, f"spatial_image_{i}.png"), bbox_inches='tight', pad_inches=0)
            plt.close()

    def get_Frequency_mask(self, input):

        # # Save the image after de-normalization
        # save_dir = "/home/jyw/SuperMan/PriViLege_Clear/models/Ouptput/images"
        # self.save_batch_as_images(input, save_dir)
        # sys.exit()

        # Perform Fourier transform on the h and w dimensions
        fft_im = torch.fft.fftn(input, dim=(-2, -1))  # 2D Fourier transform
        fft_im_center = torch.fft.fftshift(fft_im, dim=(-2, -1))  # Shift the zero frequency to the center

        # Build a grid to calculate the distance from each point to the center of the spectrum
        Batch_size, channels, h, w = input.shape
        y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')
        center_y, center_x = h // 2, w // 2  # Center of the spectrum
        distances = torch.sqrt((y - center_y) ** 2 + (x - center_x) ** 2)  # Distance matrix
        distances = distances.to(input.device)  # Ensure the device is consistent

        # Create a ring mask, allow a certain tolerance range
        beta = 4.0
        ring_masks = []  # Store the mask of each ring
        for i, radius in enumerate(self.radii):
            if i == 0:
                inner_radius = 0  # The first ring starts from the center
            else:
                inner_radius = self.radii[i - 1] + 1e-6  # Ensure no overlap

            # Outer radius mask
            outer_mask = torch.sigmoid(-beta * (distances - radius))
            # Inner radius mask
            inner_mask = torch.sigmoid(-beta * (distances - inner_radius))
            # Ring mask
            ring_mask = outer_mask - inner_mask
            ring_masks.append(ring_mask.float())  # Convert to float, for subsequent operations

        # Stack the masks into a tensor of shape [10, h, w]
        ring_masks = torch.stack(ring_masks, dim=0).to(input.device)  # [10, h, w]

        # # Save the 10 masks obtained
        # save_dir = "/home/jyw/SuperMan/PriViLege_Clear/models/Ouptput/ring_masks"
        # self.save_ring_masks_as_images(ring_masks, save_dir)
        # sys.exit()

        # Weight each ring, at this time, to run the best performance, the temperature is temporarily removed
        # weights_normalized = torch.softmax(self.weights, dim=0)  # Normalize the weights
     
        weights_normalized = torch.softmax(self.weights * self.args.temperature, dim=0)  # Normalize the weights
        weighted_ring_masks = weights_normalized[:, None, None] * ring_masks  # Weighted mask
        # Sum the weighted masks, get the overall frequency mask
        final_mask = weighted_ring_masks.sum(dim=0)  # [h, w]

        # # Save the 10 masks obtained
        # save_dir = "/home/jyw/SuperMan/PriViLege_Clear/models/Ouptput/ring_masks_weighted"
        # self.save_weighted_mask(final_mask, save_dir)
        # sys.exit() 

        # Apply the frequency mask
        fft_selected = fft_im_center * final_mask[None, None, :, :]  # 广播到 [Batch_size, 3, h, w]

        # save_dir = "/home/jyw/SuperMan/PriViLege_Clear/models/Ouptput/fft_selected_firstbatch"
        # self.save_frequency_domain_image(fft_selected, save_dir)
        # sys.exit()  # Only keep the first batch

        # (3) Use the residual operation
        fft_residual = fft_im_center + fft_selected  # Original frequency + weighted ring
        ifft_residual = torch.fft.ifftn(torch.fft.ifftshift(fft_residual, dim=(-2, -1)), dim=(-2, -1))
        ifft_residual = torch.abs(ifft_residual)  # [Batch_size, 3, h, w]
        output = input + (ifft_residual - input) * 0.1
        # Added a step, add the ring to the input frequency domain, return to the spatial domain, and then subtract the input. Get the learned information.
        # From the idea, I want to add the ring to the input frequency domain, and directly use it as the next input.
        return output

        # # (2) Operate directly in the frequency domain
        # fft_combined = fft_im_center + fft_selected  # Original frequency image and frequency ring masked frequency叠加
        # ifft_combined = torch.fft.ifftn(torch.fft.ifftshift(fft_combined, dim=(-2, -1)), dim=(-2, -1))
        # ifft_combined = torch.abs(ifft_combined)  # [Batch_size, 3, h, w]
        # return ifft_combined
    

        # # （1）返回到时域
        # ifft_selected = torch.fft.ifftn(torch.fft.ifftshift(fft_selected, dim=(-2, -1)), dim=(-2, -1))
        # ifft_selected = torch.abs(ifft_selected)  # [Batch_size, 3, h, w]

        # # save_dir = "/home/jyw/SuperMan/PriViLege_Clear/models/Ouptput/ifft_selected_firstbatch"
        # # self.save_spatial_domain_images(ifft_selected, save_dir)
        # # sys.exit()  # 只保留第一个batch的

        # return ifft_selected

    def forward(self, input, query=False, memory_data=None, session=-1):
        res = {}
        
        if self.args.adaptive_weighting:
            if self.args.pixel_prompt == 'YES':
                res = self.get_prompts(input, session=session)  
                prompts = res['prompts']
                input1 = input + prompts * 1
            if self.args.Frequency_mask:
                input2 = self.get_Frequency_mask(input)
            input = self.alpha * input1 + self.beta * input2
        else:
            if self.args.pixel_prompt == 'YES':
                res = self.get_prompts(input, session=session)  
                prompts = res['prompts']
                input = input + prompts * 1
            if self.args.Frequency_mask:
                input = self.get_Frequency_mask(input)

        if query:
            q_feat = self.encode(input)
            return q_feat

        embedding = self.prompt_encode(input)
        logit = self.classifier_head(embedding)

        res['logit'] = logit

        if memory_data is not None:  # 把通过高斯分布得到数据拿到
            logit = torch.cat([logit, memory_data], dim=0)
        return res

    def train_inc(self, dataloader, epochs, session, class_list, testloader, result_list, test, model_test):
        print("[Session: {}]".format(session))
        self.update_fc_avg(dataloader, class_list)
        optimizer_params = []

        if self.args.Frequency_mask: # 在novel阶段也优化权重
            params_Frequency_mask = [self.weights]
            optimizer_params.append({'params': params_Frequency_mask, 'lr': self.args.lr_Frequency_mask * 0.05})

        # VPT
        params_vpt = [self.Prompt_Tokens]
        optimizer_params.append({'params': params_vpt, 'lr': self.args.lr_PromptTokens_novel})

        # Classifier
        params_classsifier = [p for p in self.classifier_head.parameters()]
        optimizer_params.append({'params': params_classsifier, 'lr': self.args.lr_new})

        optim = torch.optim.Adam(optimizer_params)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=epochs * 1)
        
        best_epoch = -1
        best_accuracy = 0.0

        last_novel_acc = 0.0

        for epoch in range(epochs):
            batch_id = -1
            for idx,batch in enumerate(dataloader):
                data_imgs, data_label = [_.cuda() for _ in batch]

                sg_inputs=None
        
                self.train()

                res = self.forward(data_imgs, memory_data=sg_inputs, session=session)
                logits = res['logit']

                seen_class = self.base_class + session * self.way
                logits = logits[:, :seen_class] # Same as test

                loss_ce = F.cross_entropy(logits, data_label)
                loss = loss_ce
                
                optim.zero_grad()
                loss.backward()
                
                optim.step()
                scheduler.step()
                pred = torch.argmax(logits, dim=1)
                acc = (pred == data_label).sum().item()/data_label.shape[0]*100.
                
            lrc = scheduler.get_last_lr()[0]  # Get the current learning rate
            tsl, tsa, logs = test(model_test, testloader, self.args, session)
            last_novel_acc = logs.get('new_acc', 0.0) * 100.0
            if tsa > best_accuracy:
                best_accuracy = tsa
                best_epoch = epoch

            result_list.append(
                        'epoch:%03d,lr:%.4f,B:%.5f,N:%.5f,BN:%.5f,NB:%.5f,training_loss:%.5f,training_acc:%.5f,test_loss:%.5f,test_acc:%.5f' % (
                            epoch, lrc, logs['base_acc'], logs['new_acc'], logs['base_acc_given_new'], logs['new_acc_given_base'], loss, acc, tsl, tsa
                        )
                    )
        result_list.append('Session {}, Best test_Epoch {}, Best test_Acc {:.4f}'.format(
                    session, best_epoch, best_accuracy))

        return tsa, last_novel_acc
    
    def update_fc_avg(self,dataloader,class_list):
        self.eval()
        query_p=[]
        
        with torch.no_grad():
            for batch in dataloader:
                data_imgs, label = [_.cuda() for _ in batch]
                cls_embed=self.encode(data_imgs).detach()
            
            for class_index in class_list:
                data_index=(label==class_index).nonzero().squeeze(-1)
                embedding = cls_embed[data_index]
                proto=embedding.mean(0)
                query_p.append(proto)
                self.classifier_head.weight.data[class_index]=proto
            query_p = torch.stack(query_p)
        # query_info["proto"] = torch.cat([query_info["proto"], query_p.cpu()])
        
        self.train()